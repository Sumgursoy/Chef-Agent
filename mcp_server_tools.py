"""
ChefAgent - MCP Server Tools
DuckDuckGo ile yemek tarifi arama ve Crawl4AI ile web scraping araÃ§larÄ±.
"""

import asyncio
import json
import logging
from typing import Optional

import redis.asyncio as aioredis
from ddgs import DDGS
from crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig

from config import REDIS_HOST, REDIS_PORT, REDIS_DB, REDIS_SCRAPE_CACHE_TTL

logger = logging.getLogger("chef_agent.tools")

# â”€â”€â”€ Redis baÄŸlantÄ±sÄ± (lazy init) â”€â”€â”€
_redis_client: Optional[aioredis.Redis] = None


async def get_redis() -> aioredis.Redis:
    """Redis baÄŸlantÄ±sÄ±nÄ± lazy olarak oluÅŸturur ve dÃ¶ner."""
    global _redis_client
    if _redis_client is None:
        _redis_client = aioredis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            decode_responses=True,
        )
    return _redis_client


async def close_redis() -> None:
    """Redis baÄŸlantÄ±sÄ±nÄ± kapatÄ±r."""
    global _redis_client
    if _redis_client:
        await _redis_client.close()
        _redis_client = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tool 1: search_recipes  â€“  DuckDuckGo ile yemek tarifi arama
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def search_recipes(query: str, max_results: int = 5) -> str:
    """
    DuckDuckGo Ã¼zerinden yemek tarifi arar.

    Args:
        query: Aranacak yemek tarifi sorgusu (Ã¶rn. "karnÄ±yarÄ±k tarifi").
        max_results: DÃ¶ndÃ¼rÃ¼lecek maksimum sonuÃ§ sayÄ±sÄ± (varsayÄ±lan: 5).

    Returns:
        JSON formatÄ±nda arama sonuÃ§larÄ± listesi. Her sonuÃ§ title, url ve snippet iÃ§erir.
    """
    try:
        logger.info(f"ğŸ” DuckDuckGo aramasÄ±: '{query}' (max: {max_results})")

        # "tarifi" anahtar kelimesini ekleyerek yemek odaklÄ± arama yap
        search_query = f"{query} tarifi"

        # DDGS sync olduÄŸu iÃ§in executor'da Ã§alÄ±ÅŸtÄ±rÄ±yoruz
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            None,
            lambda: list(DDGS().text(search_query, max_results=max_results))
        )

        # SonuÃ§larÄ± dÃ¼zenle
        formatted_results = []
        for r in results:
            formatted_results.append({
                "title": r.get("title", ""),
                "url": r.get("href", ""),
                "snippet": r.get("body", ""),
            })

        result_json = json.dumps(formatted_results, ensure_ascii=False, indent=2)
        logger.info(f"âœ… {len(formatted_results)} sonuÃ§ bulundu")
        return result_json

    except Exception as e:
        error_msg = f"Arama sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return json.dumps({"error": error_msg}, ensure_ascii=False)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tool 2: scrape_recipe  â€“  Crawl4AI ile tarif sayfasÄ±nÄ± Ã§ekme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def scrape_recipe(url: str) -> str:
    """
    Verilen URL'deki yemek tarifi sayfasÄ±nÄ± Markdown formatÄ±nda Ã§eker.
    SonuÃ§lar Redis'te cache'lenir (TTL: 1 saat).

    Args:
        url: Tarif sayfasÄ±nÄ±n URL'si.

    Returns:
        SayfanÄ±n Markdown formatÄ±nda iÃ§eriÄŸi.
    """
    try:
        logger.info(f"ğŸŒ Sayfa Ã§ekiliyor: {url}")

        # â”€â”€ Redis cache kontrolÃ¼ â”€â”€
        redis_client = await get_redis()
        cache_key = f"scrape:{url}"

        cached = await redis_client.get(cache_key)
        if cached:
            logger.info("ğŸ’¾ Cache'den dÃ¶ndÃ¼rÃ¼lÃ¼yor")
            return cached

        # â”€â”€ Crawl4AI ile sayfayÄ± Ã§ek â”€â”€
        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=config)

            if not result.success:
                error_msg = f"Sayfa Ã§ekilemedi: {result.error_message}"
                logger.error(f"âŒ {error_msg}")
                return json.dumps({"error": error_msg}, ensure_ascii=False)

            # Markdown iÃ§eriÄŸini al (Ã§ok uzunsa kÄ±rp)
            markdown_content = result.markdown or ""
            if len(markdown_content) > 8000:
                markdown_content = markdown_content[:8000] + "\n\n... (iÃ§erik kÄ±rpÄ±ldÄ±)"

            # â”€â”€ Redis'e cache'le â”€â”€
            await redis_client.setex(cache_key, REDIS_SCRAPE_CACHE_TTL, markdown_content)
            logger.info(f"âœ… Sayfa baÅŸarÄ±yla Ã§ekildi ({len(markdown_content)} karakter)")

            return markdown_content

    except Exception as e:
        error_msg = f"Sayfa Ã§ekme sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return json.dumps({"error": error_msg}, ensure_ascii=False)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tool Registry  â€“  AraÃ§ adÄ±ndan fonksiyona eÅŸleme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TOOL_FUNCTIONS = {
    "search_recipes": search_recipes,
    "scrape_recipe": scrape_recipe,
}


async def execute_tool(name: str, args: dict) -> str:
    """
    AraÃ§ adÄ±nÄ± ve argÃ¼manlarÄ±nÄ± alÄ±r, ilgili fonksiyonu Ã§alÄ±ÅŸtÄ±rÄ±r.

    Args:
        name: AraÃ§ adÄ± ("search_recipes" veya "scrape_recipe").
        args: AraÃ§ fonksiyonuna geÃ§ilecek argÃ¼manlar.

    Returns:
        AraÃ§ fonksiyonunun dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ string sonuÃ§.
    """
    func = TOOL_FUNCTIONS.get(name)
    if func is None:
        return json.dumps({"error": f"Bilinmeyen araÃ§: {name}"}, ensure_ascii=False)

    logger.info(f"ğŸ› ï¸  AraÃ§ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor: {name}({args})")
    return await func(**args)
